{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save padded image\n",
    "Given an image path (`im_pth`), the code below will extract its file name (`file_name`) and generate the following images in the `transformed_images` directory:\n",
    "* unpadded original image: `{file_name}_no_pad.png`\n",
    "* padded image to fit into square dimensions: `{file_name}_pad.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "img_size = 2048\n",
    "im_pth = '/home/aisinai/data/mimic/valid/p10296197/s03/view2_lateral.jpg'  # change to your image path\n",
    "base = os.path.basename(im_pth)\n",
    "file_name = os.path.splitext(base)[0]\n",
    "\n",
    "os.makedirs('transformed_images', exist_ok=True)\n",
    "\n",
    "im = Image.open(im_pth)\n",
    "im.save(f'transformed_images/{file_name}_no_pad.png')\n",
    "\n",
    "old_size = im.size  # old_size[0] is in (width, height) format\n",
    "ratio = float(img_size) / max(old_size)\n",
    "new_size = tuple([int(x * ratio) for x in old_size])\n",
    "im = im.resize(new_size, Image.ANTIALIAS)\n",
    "\n",
    "# create a new image for padding and paste the resized on it\n",
    "new_im = Image.new(\"RGB\", (img_size, img_size))\n",
    "new_im.paste(im, ((img_size - new_size[0]) // 2,\n",
    "                  (img_size - new_size[1]) // 2))\n",
    "new_im.save(f'transformed_images/{file_name}_pad.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save reconstructed images\n",
    "There are 4 models, identified by their train runs. First three models have encoder output depth of 64.\n",
    "* 0: Model A. 2 convolutions in the first / bottom layer | 1 convolution  in the second / top layer\n",
    "* 1: Model B. 3 convolutions in the first / bottom layer | 2 convolutions in the second / top layer\n",
    "* 3: Model C. 4 convolutions in the first / bottom layer | 2 convolutions in the second / top layer\n",
    "\n",
    "Last model, Model D, have encoder output depth of 1.\n",
    "* embed1: Model D. 2 convolutions in the first / bottom layer | 1 convolution in the second / top layer\n",
    "\n",
    "It takes as the input the padded image in the `transformed_images` directory from the above code block and generate the following images in the `transformed_images` directory: \n",
    "* `{file_name}_original.png`\n",
    "* Output from Model A: `{file_name}_recon_A.png`\n",
    "* Output from Model B: `{file_name}_recon_B.png`\n",
    "* Output from Model C: `{file_name}_recon_C.png`\n",
    "* Output from Model D: `{file_name}_recon_D.png`\n",
    "\n",
    "Note that the images are converted to grayscale with the formula `gray = 0.2989 * r + 0.5870 * g + 0.1140 * b`\n",
    "to eliminate the blue tint that results from plotting the RGB output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.ranking module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from networks import VQVAE\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from utilities import rgb2gray\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "normalization = transforms.Normalize(mean=mean, std=std)\n",
    "transform_array = [transforms.Resize(img_size), transforms.CenterCrop(img_size), transforms.ToTensor(), normalization]\n",
    "transform = transforms.Compose(transform_array)\n",
    "\n",
    "image = torch.zeros((1, 3, img_size, img_size))  # img_size from above\n",
    "image[0, :] = transform(Image.open(f'transformed_images/{file_name}_pad.png').convert('RGB'))  # file_name from above\n",
    "\n",
    "mean = torch.FloatTensor([0.485, 0.456, 0.406]).reshape(3, 1, 1).type(Tensor)\n",
    "std = torch.FloatTensor([0.229, 0.224, 0.225]).reshape(3, 1, 1).type(Tensor)\n",
    "\n",
    "for model_name in ['A', 'B', 'C', 'D']:\n",
    "    if model_name == 'A':\n",
    "        # model_dir = path to {saved_model}.pt checkpoint file for model A\n",
    "        model_dir = '/home/aisinai/work/VQ-VAE2/20200422/vq_vae/CheXpert/0/checkpoint/vqvae_040.pt'\n",
    "        model = VQVAE(first_stride=4, second_stride=2).cuda() if cuda else VQVAE()\n",
    "    elif model_name == 'B':\n",
    "        # model_dir = path to {saved_model}.pt checkpoint file for model B\n",
    "        model_dir = '/home/aisinai/work/VQ-VAE2/20200422/vq_vae/CheXpert/1/checkpoint/vqvae_040.pt'\n",
    "        model = VQVAE(first_stride=8, second_stride=4).cuda() if cuda else VQVAE()\n",
    "    elif model_name == 'C':\n",
    "        # model_dir = path to {saved_model}.pt checkpoint file for model C\n",
    "        model_dir = '/home/aisinai/work/VQ-VAE2/20200422/vq_vae/CheXpert/3/checkpoint/vqvae_040.pt'\n",
    "        model = VQVAE(first_stride=16, second_stride=4).cuda() if cuda else VQVAE()\n",
    "    elif model_name == 'D':\n",
    "        # model_dir = path to {saved_model}.pt checkpoint file for model D\n",
    "        model_dir = '/home/aisinai/work/VQ-VAE2/20200422/vq_vae/CheXpert/embed1/checkpoint/vqvae_040.pt'\n",
    "        model = VQVAE(first_stride=4, second_stride=2, embed_dim=1).cuda() if cuda else VQVAE()\n",
    "\n",
    "    model.load_state_dict(torch.load(model_dir))\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    if n_gpu > 1:\n",
    "        device_ids = list(range(n_gpu))\n",
    "        model = nn.DataParallel(model, device_ids=device_ids)\n",
    "    model.eval()\n",
    "    original_img = Variable(image.type(Tensor))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out, _ = model(original_img)\n",
    "        decoded_img, _ = model(original_img)\n",
    "        quant_t, quant_b, _, id_t, id_b = model.encode(original_img)\n",
    "        upsample_t = model.upsample_t(quant_t)\n",
    "        quant = torch.cat([upsample_t, quant_b], 1)\n",
    "\n",
    "    original_img = original_img * std - mean\n",
    "    out = out * std - mean\n",
    "    save_image(rgb2gray(original_img[0,:]).data,\n",
    "               f'transformed_images/{file_name}_original.png', \n",
    "               nrow=1, normalize=True, range=(-1, 1))\n",
    "    save_image(rgb2gray(out[0,:]).data,\n",
    "               f'transformed_images/{file_name}_recon_{model_name}.png',\n",
    "               nrow=1, normalize=True, range=(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute PSNR\n",
    "Take two directories, one containing the original images and the other the reconstructed images, and compute PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR value for model A is 45.39284165046035 dB\n",
      "Average PSNR value for model B is 44.05189311629433 dB\n",
      "Average PSNR value for model C is 41.590470862564146 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from math import log10, sqrt\n",
    "\n",
    "orig_dir = '/home/aisinai/work/VQ-VAE2-Images/1024/frontal/original'\n",
    "recon_dir = '/home/aisinai/work/VQ-VAE2-Images/1024/frontal'\n",
    "\n",
    "def PSNR(original, compressed):\n",
    "    mse = np.mean((original - compressed) ** 2)\n",
    "    if(mse == 0):  # MSE is zero means no noise is present in the signal.\n",
    "                   # Therefore PSNR have no importance.\n",
    "        return 100\n",
    "    max_pixel = 255.0\n",
    "    psnr = 20 * log10(max_pixel / sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "PSNRs = []\n",
    "for model in ['A', 'B', 'C']:\n",
    "    for image in os.listdir(orig_dir):\n",
    "        original = np.asarray(Image.open(f'{orig_dir}/{image}').convert('RGB'))\n",
    "        recon = np.asarray(Image.open(f'{recon_dir}/{model}/{image}').convert('RGB'))\n",
    "        PSNRs.append(PSNR(original, recon))\n",
    "    print(f'Average PSNR value for model {model} is {np.average(PSNRs)} dB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save VQ-VAE-2 Models as ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model A: saved to model_A.onnx\n",
      "Model B: saved to model_B.onnx\n",
      "Model C: saved to model_C.onnx\n",
      "Model D: saved to model_D.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from networks import VQVAE\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "img_size = 1024\n",
    "batch_size = 1\n",
    "num_channel = 3  # 3 for RGB\n",
    "dummy_image = torch.randn(batch_size, num_channel, img_size, img_size, requires_grad=True)\n",
    "\n",
    "for model_name in ['A', 'B', 'C', 'D']:\n",
    "    if model_name == 'A':\n",
    "        # model_dir = path to {saved_model}.pt checkpoint file for model A\n",
    "        model_dir = '/home/aisinai/work/VQ-VAE2/20200422/vq_vae/CheXpert/0/checkpoint/vqvae_040.pt'\n",
    "        model = VQVAE(first_stride=4, second_stride=2)\n",
    "    elif model_name == 'B':\n",
    "        # model_dir = path to {saved_model}.pt checkpoint file for model B\n",
    "        model_dir = '/home/aisinai/work/VQ-VAE2/20200422/vq_vae/CheXpert/1/checkpoint/vqvae_040.pt'\n",
    "        model = VQVAE(first_stride=8, second_stride=4)\n",
    "    elif model_name == 'C':\n",
    "        # model_dir = path to {saved_model}.pt checkpoint file for model C\n",
    "        model_dir = '/home/aisinai/work/VQ-VAE2/20200422/vq_vae/CheXpert/3/checkpoint/vqvae_040.pt'\n",
    "        model = VQVAE(first_stride=16, second_stride=4)\n",
    "    elif model_name == 'D':\n",
    "        # model_dir = path to {saved_model}.pt checkpoint file for model D\n",
    "        model_dir = '/home/aisinai/work/VQ-VAE2/20200422/vq_vae/CheXpert/embed1/checkpoint/vqvae_040.pt'\n",
    "        model = VQVAE(first_stride=4, second_stride=2, embed_dim=1)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_dir))\n",
    "    model.eval()\n",
    "    out = model(dummy_image)\n",
    "    torch.onnx.export(model, dummy_image, f'model_{model_name}.onnx')\n",
    "    print(f'Model {model_name}: saved to model_{model_name}.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save DenseNet-121 Models as ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model orig: saved to model_orig.onnx\n",
      "Model recon: saved to model_recon.onnx\n",
      "Model latent: saved to model_latent.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from networks import Densenet121\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "img_size = 256\n",
    "batch_size = 1\n",
    "\n",
    "dummy_input = torch.randn(batch_size, num_channel, img_size, img_size, requires_grad=True)\n",
    "n_classes = 14\n",
    "\n",
    "for model_name in ['orig', 'recon', 'latent']:\n",
    "    if model_name == 'orig':\n",
    "        # model_dir = path to {saved_model}.pt checkpoint file for original image inputs\n",
    "        model_dir = '/home/aisinai/work/VQ-VAE2/20200424/densenet121/orig/best_densenet_model.pt'\n",
    "        model = Densenet121(n_classes=n_classes, input_type=model_name)\n",
    "        model.model.load_state_dict(torch.load(model_dir))\n",
    "        num_channel = 3  # 3 for RGB\n",
    "        dummy_input = torch.randn(batch_size, num_channel, img_size, img_size, requires_grad=True)\n",
    "    elif model_name == 'recon':\n",
    "        # model_dir = path to {saved_model}.pt checkpoint file for reconstructed image inputs\n",
    "        model_dir = '/home/aisinai/work/VQ-VAE2/20200424/densenet121/recon/best_densenet_model.pt'\n",
    "        model = Densenet121(n_classes=n_classes, input_type=model_name)\n",
    "        model.model.load_state_dict(torch.load(model_dir))\n",
    "        num_channel = 3  # 3 for RGB\n",
    "        dummy_input = torch.randn(batch_size, num_channel, img_size, img_size, requires_grad=True)\n",
    "    elif model_name == 'latent':\n",
    "        # model_dir = path to {saved_model}.pt checkpoint file for latent vector inputs\n",
    "        model_dir = '/home/aisinai/work/VQ-VAE2/20200424/densenet121/latent/best_densenet_model.pt'\n",
    "        model = Densenet121(n_classes=n_classes, input_type=model_name)\n",
    "        num_channel = 2  # 2 for 2 latent vectors concatenated\n",
    "        dummy_input = torch.randn(batch_size, num_channel, img_size, img_size, requires_grad=True)\n",
    "        model.load_state_dict(torch.load(model_dir))\n",
    "\n",
    "    model.eval()\n",
    "    out = model(dummy_input)\n",
    "    torch.onnx.export(model, dummy_input, f'densenet_{model_name}.onnx')\n",
    "    print(f'Model {model_name}: saved to model_{model_name}.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
