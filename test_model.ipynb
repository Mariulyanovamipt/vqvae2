{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save padded image\n",
    "Given an image path (`im_pth`), the code below will extract its file name (`file_name`) and generate the following images in the `transformed_images` directory:\n",
    "* unpadded original image: `{file_name}_no_pad.png`\n",
    "* padded image to fit into square dimensions: `{file_name}_pad.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "img_size = 2048\n",
    "im_pth = '/home/aisinai/data/mimic/valid/p10228846/s01/view1_frontal.jpg'  # change to your image path\n",
    "base = os.path.basename(im_pth)\n",
    "file_name = os.path.splitext(base)[0]\n",
    "\n",
    "os.makedirs('transformed_images', exist_ok=True)\n",
    "\n",
    "im = Image.open(im_pth)\n",
    "im.save(f'transformed_images/{file_name}_no_pad.png')\n",
    "\n",
    "old_size = im.size  # old_size[0] is in (width, height) format\n",
    "ratio = float(img_size) / max(old_size)\n",
    "new_size = tuple([int(x * ratio) for x in old_size])\n",
    "im = im.resize(new_size, Image.ANTIALIAS)\n",
    "\n",
    "# create a new image for padding and paste the resized on it\n",
    "new_im = Image.new(\"RGB\", (img_size, img_size))\n",
    "new_im.paste(im, ((img_size - new_size[0]) // 2,\n",
    "                  (img_size - new_size[1]) // 2))\n",
    "new_im.save(f'transformed_images/{file_name}_pad.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save reconstructed images\n",
    "There are 4 models, identified by their train runs. First three models have encoder output depth of 64.\n",
    "* 0: Model A. 2 convolutions in the first / bottom layer | 1 convolution  in the second / top layer\n",
    "* 1: Model B. 3 convolutions in the first / bottom layer | 2 convolutions in the second / top layer\n",
    "* 3: Model C. 4 convolutions in the first / bottom layer | 2 convolutions in the second / top layer\n",
    "\n",
    "Last model, Model D, have encoder output depth of 1.\n",
    "* embed1: Model D. 2 convolutions in the first / bottom layer | 1 convolution in the second / top layer\n",
    "\n",
    "It takes as the input the padded image in the `transformed_images` directory from the above code block and generate the following images in the `transformed_images` directory: \n",
    "* `{file_name}_original.png`\n",
    "* Output from Model A: `{file_name}_recon_A.png`\n",
    "* Output from Model B: `{file_name}_recon_B.png`\n",
    "* Output from Model C: `{file_name}_recon_C.png`\n",
    "* Output from Model D: `{file_name}_recon_D.png`\n",
    "\n",
    "Note that the images are converted to grayscale with the formula `gray = 0.2989 * r + 0.5870 * g + 0.1140 * b`\n",
    "to eliminate the blue tint that results from plotting the RGB output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.ranking module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from networks import VQVAE\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from utilities import rgb2gray\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "normalization = transforms.Normalize(mean=mean, std=std)\n",
    "transform_array = [transforms.Resize(img_size), transforms.CenterCrop(img_size), transforms.ToTensor(), normalization]\n",
    "transform = transforms.Compose(transform_array)\n",
    "\n",
    "image = torch.zeros((1, 3, img_size, img_size))  # img_size from above\n",
    "image[0, :] = transform(Image.open(f'transformed_images/{file_name}_pad.png').convert('RGB'))  # file_name from above\n",
    "\n",
    "mean = torch.FloatTensor([0.485, 0.456, 0.406]).reshape(3, 1, 1).type(Tensor)\n",
    "std = torch.FloatTensor([0.229, 0.224, 0.225]).reshape(3, 1, 1).type(Tensor)\n",
    "\n",
    "for model_name in ['A', 'B', 'C', 'D']:\n",
    "    if model_name == 'A':\n",
    "        model_dir = '/home/aisinai/work/VQ-VAE2/20200422/vq_vae/CheXpert/0/checkpoint/vqvae_040.pt'\n",
    "        model = VQVAE(first_stride=4, second_stride=2).cuda() if cuda else VQVAE()\n",
    "    elif model_name == 'B':\n",
    "        model_dir = '/home/aisinai/work/VQ-VAE2/20200422/vq_vae/CheXpert/1/checkpoint/vqvae_040.pt'\n",
    "        model = VQVAE(first_stride=8, second_stride=4).cuda() if cuda else VQVAE()\n",
    "    elif model_name == 'C':\n",
    "        model_dir = '/home/aisinai/work/VQ-VAE2/20200422/vq_vae/CheXpert/3/checkpoint/vqvae_040.pt'\n",
    "        model = VQVAE(first_stride=16, second_stride=4).cuda() if cuda else VQVAE()\n",
    "    elif model_name == 'D':\n",
    "        model_dir = '/home/aisinai/work/VQ-VAE2/20200422/vq_vae/CheXpert/embed1/checkpoint/vqvae_040.pt'\n",
    "        model = VQVAE(first_stride=4, second_stride=2, embed_dim=1).cuda() if cuda else VQVAE()\n",
    "\n",
    "    model.load_state_dict(torch.load(model_dir))\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    if n_gpu > 1:\n",
    "        device_ids = list(range(n_gpu))\n",
    "        model = nn.DataParallel(model, device_ids=device_ids)\n",
    "    model.eval()\n",
    "    original_img = Variable(image.type(Tensor))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out, _ = model(original_img)\n",
    "        decoded_img, _ = model(original_img)\n",
    "        quant_t, quant_b, _, id_t, id_b = model.encode(original_img)\n",
    "        upsample_t = model.upsample_t(quant_t)\n",
    "        quant = torch.cat([upsample_t, quant_b], 1)\n",
    "\n",
    "    original_img = original_img * std - mean\n",
    "    out = out * std - mean\n",
    "    save_image(rgb2gray(original_img[0,:]).data,\n",
    "               f'transformed_images/{file_name}_original.png', \n",
    "               nrow=1, normalize=True, range=(-1, 1))\n",
    "    save_image(rgb2gray(out[0,:]).data,\n",
    "               f'transformed_images/{file_name}_recon_{model_name}.png',\n",
    "               nrow=1, normalize=True, range=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
